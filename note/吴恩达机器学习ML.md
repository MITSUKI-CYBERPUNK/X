# 吴恩达机器学习ML

# 目录

[TOC]

# 1 概述

## 1.1 你好世界

## 1.2 机器学习应用



# 2 有/无监督学习

## 2.1 机器学习定义

> 使计算机无需明确编程即可学习的研究领域。——Arthur Samuel



> 一个程序被认为能从经验**E**中学习，解决任务**T**，达到性能度量值**P**。当且仅当，有了经验**E**后，经过**P**评判，程序在处理T时的性能有所提升。我认为经验**E** 就是程序上万次的自我练习的经验，而任务**T** 就是下棋。性能度量值**P**呢，就是它在与一些新的对手比赛时，赢得比赛的概率。——Tom Mitchell



### 2.1.1 机器学习的类型

+ **监督学习(Supervised learning)**：实际运用使用最多，最先进

+ **无监督学习(Unsupervised learning)**

+ **强化学习(Reinforcement learning)**：应用较少



## 2.2 监督学习Supervised Learning

### 2.2.1 定义

**给定X Y使得模型学习后可以对给定的任意X得到对应的Y**

与数学的相类似

eg.根据房屋大小预测房价

可用直线或者曲线进行拟合



### 2.2.2 回归Regression——无限多的可能

eg.根据房屋大小预测房价

可用直线或者曲线进行拟合

![image-20240126215234389](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240126215234389.png)

监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做**回归问题**。我们试着推测出一个连续值的结果，即房子的价格。

一般房子的价格会记到美分，所以房价实际上是一系列离散的值，但是我们通常又把房价看成实数，看成是标量，所以又把它看成一个连续的数值。

回归这个词的意思是，**我们在试着推测出这一系列连续值属性**。



### 2.2.3 分类Classification——有限多的输出

eg.推测乳腺癌良性与否

![image-20240126215640958](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240126215640958.png)

横轴表示肿瘤的大小，纵轴1和0表示是或者不是恶性肿瘤

eg.估算肿瘤是恶性的或是良性的概率。用术语来讲，这是一个**分类问题**。

**分类**指的是，我们试着推测出离散的输出值：0或1良性或恶性，而事实上在分类问题中，输出可能不止两个值。比如说可能有三种乳腺癌，所以你希望预测离散输出0、1、2、3。0 代表良性，1 表示第1类乳腺癌，2表示第2类癌症，3表示第3类，但这也是分类问题。

我用不同的符号来表示良性和恶性肿瘤。或者说是负样本和正样本。现在我们不全部画**X**，良性的肿瘤改成用 **O** 表示，恶性的继续用 **X** 表示。来预测肿瘤的恶性与否。

在其它一些机器学习问题中，可能会遇到**不止一种特征**。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，我们通常有更多的特征，我朋友研究这个问题时，通常采用这些特征，比如肿块密度，肿瘤细胞尺寸的一致性和形状的一致性等等，还有一些其他的特征。这就是我们即将学到最有趣的学习算法之一。

那种算法不仅能处理2种3种或5种特征，即使有**无限多种特征都可以处理**。

![image-20240126215945605](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240126215945605.png)

找到一根“**边界线**”，来划分不同肿瘤情况

上图中，我列举了总共5种不同的特征，坐标轴上的两种和右边的3种，但是在一些学习问题中，你希望不只用3种或5种特征。相反，你想用**无限多**种特征，好让你的算法可以利用大量的特征，或者说线索来做推测。如何处理无限特征？**我们以后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。**想象一下，我没有写下这两种和右边的三种特征，而是在一个无限长的列表里面，一直写一直写不停的写，写下无限多个特征，事实上，我们能用算法来处理它们。



### 2.2.4 总结

现在来回顾一下，这节课我们介绍了监督学习。其基本思想是，**我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测**，就像房子和肿瘤的例子中做的那样。我们还介绍了**回归问题，即通过回归来推出一个连续的输出**，之后我们介绍了**分类问题，其目标是推出一组离散的结果。(结果连续与否)**



## 2.3 无监督学习Unsupervised Learning

### 2.3.1 特点与区别

![image-20240127204154443](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240127204154443.png)![image-20240127204217120](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240127204217120.png)

**监督学习:每个示例都与一个输出标签y相关联**，例如良性与恶性

**无监督学习:给定的数据与任何输出标签y无关**（仅输入 不输出），例如给定了有关患者及其肿瘤大小关于患者年龄的数据，不要求良性或者恶性，因此没有得到任何标签。我们的工作是**不试图监督算法，找到一些结构或模式，或者只是在数据中找到一些有趣的东西。**

**No Right Answer**



### 2.3.2 聚类Clustering

将相似的数据点组合在一起

无监督学习算法可能会把这些肿瘤数据分成两个不同的簇（但并不追究肿瘤种类）。所以叫做**聚类算法**。事实证明，它能被用在很多地方。

聚类应用：

+ 谷歌新闻搜索非常多的新闻事件，自动地把它们聚类到一起。这些新闻事件全是**同一主题**的，这就是"**聚**"。而且该算法在没有监督的情况下自行计算出今天的新闻文章有哪些并聚合。
+ 聚类之于基因学：**把个体聚类到不同的类或不同类型的组**。我们没有提前告知算法一些信息（聚类标准和数据里面的什么）因为**我们没有给算法正确答案来回应数据集中的数据**，所以这就是无监督学习。
+ 组织大型计算机集群：分类使机器协同工作
+ 社交网络的分析:划分用户，圈子等
+ 细分市场分类  



### 2.3.3  异常检测Anomaly Detection

**用于检测异常事件**

eg.金融系统的欺诈检测，关注异常事件。 



### 2.3.4 降维 Dimensionality Reduction

**将一个大数据集神奇地压缩成一个小得多的数据集，同时丢失尽可能少的信息。**



# 3 单变量线性回归Univariate Linear Regression和代价(成本 )函数Cost Function

## 3.1 单变量线性回归Univariate Linear

eg.根据房屋大小预测住房价格

![image-20240130094047742](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130094047742.png)

这是一个监督学习的线性回归问题。我们的数据集称为训练集。

![image-20240130095058677](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130095058677.png)



## 3.1.1 训练集标准符号

**整个课程中将用小写的$m$来表示训练样本的数目**

**m 代表训练集中实例的数量**

**x 代表特征/输入变量** input/feature

**y 代表目标变量/输出变量** output/target

**(x,y) 代表训练集中的实例**

**(x^(i)^,y^(i)^)代表第$i$ 个观察实例**

**代表学习算法的解决方案或函数也称为假设（hypothesis）**

![image-20240130100808795](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130100808795.png)

$h$表示一个函数，输入是房屋尺寸大小，根据输入的$x$值来得出$y$值，$y$ 值对应房子的价格 因此，$h$ 是一个从$x$到$y$的函数映射。

要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设$h$，然后将我们要预测的房屋的尺寸作为输入变量输入给$h$，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达$h$ ？

一种可能的表达方式为：$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$，因为只**含有一个特征/输入变量**，因此这样的问题叫作**单变量线性回归问题**。



**y是目标，训练集中的实际真实值**

**y-hat是对y的估计或预测**



## 3.2 代价（成本）函数Cost Function

### 3.3.1 数学定义

在线性回归的基础上，我们继续研究：

![image-20240130100838357](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130100838357.png)

我们现在要做的便是为我们的模型选择合适的**参数**（**parameters**）$\theta_{0}$ 和 $\theta_{1}$，在房价问题这个例子中便是直线的斜率k和在$y$ 轴上的截距b。

我们选择的**参数决定了我们得到的直线相对于我们的训练集的准确程度**，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）。**可以通过不断调整改进参数来减小建模误差。**

![image-20240130101033459](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101033459.png)

我们的**目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小。**（机器学习中再/2使计算更为简洁，此时成本函数仍然有效）



**参数（Parameters）有时也被称为系数（Coefficients）或权重（Weights）。**

我们绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$：

![image-20240130101451864](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101451864.png)

则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点。

**代价函数也被称作平方误差函数，有时也被称为平方误差代价函数**。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段。



### 3.3.2 代价函数的直观理解

![image-20240130101912818](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101912818.png)

代价函数的样子:

 ![image-20240130104110485](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130104110485.png)

3D碗形曲面图：碗底使代价函数最小，最适合预测

![image-20240130101451864](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101451864.png)

等值线图：（对3D碗形曲面图底部取3D表面并水平切片）

![image-20240130102027703](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130102027703.png)

通过这些图形，我希望你能更好地理解这些代价函数$ J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数$J$的最小值。

当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数$\theta_{0}$和$\theta_{1}$来。

我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的$\theta_{0}$和$\theta_{1}$的值，在下一节视频中，我们将介绍一种算法，能够**自动地找出能使代价函数$J$最小化的参数$\theta_{0}$和$\theta_{1}$的值**。



# 4 梯度下降Gradient Descent

## 4.1 数学定义

梯度下降是一个用来**求函数最小值**的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_{0}, \theta_{1})$ 的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合$\left(  {\theta_{0}},{\theta_{1}},......,{\theta_{n}}  \right)$，计算代价函数，然后我们寻找下一个**能让代价函数值下降最多的参数组合**。我们持续这么做直到找到一个局部最小值（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。

![image-20240130120707450](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130120707450.png)

想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。

**批量梯度下降（batch gradient descent）算法的公式为：**

![image-20240130120804477](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130120804477.png)

其中$a$是**学习率（learning rate）**，通常是0到1之间的一个小正数。它**决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大**，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数(进行少量调整，决定了方向以及步数大小)。

**等号可表示赋值或者真值断言，此处是赋值。**

![image-20240130120820778](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130120820778.png)

在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新${\theta_{0}}$和${\theta_{1}}$ ，当 $j = 0$和$j=1$时，会产生更新，所以你将更新$J\left( {\theta_{0}} \right)$和$J\left(  {\theta_{1}}  \right)$。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要同时更新${\theta_{0}}$和${\theta_{1}}$，我的意思是在这个等式中，我们要这样更新：

${\theta_{0}}$:= ${\theta_{0}}$

并更新

${\theta_{1}}$:= ${\theta_{1}}$

实现方法是：**你应该计算公式右边的部分，通过那一部分计算出${\theta_{0}}$和${\theta_{1}}$的值，然后同时更新${\theta_{0}}$和${\theta_{1}}$。**

进一步阐述这个过程：

![image-20240130121059884](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130121059884.png)

在梯度下降算法中，这是正确实现同时更新的方法。千万不要带入！我不打算解释为什么你需要**同时更新**，同时更新是梯度下降中的一种常用方法。我们之后会讲到，**同步更新是更自然的实现方法。当人们谈到梯度下降时，他们的意思就是同步更新**。

在接下来的视频中，我们要进入这个微分项的细节之中。我已经写了出来但没有真正定义，如果你已经修过微积分课程，如果你熟悉偏导数和导数，这其实就是这个微分项：

**$\alpha \frac{\partial }{\partial {{\theta }*{0}}}J({{\theta }*{0}},{{\theta }*{1}})$，$\alpha \frac{\partial }{\partial {{\theta }*{1}}}J({{\theta }*{0}},{{\theta }*{1}})$**



## 4.2 梯度下降的直观理解

在之前的视频中，我们给出了一个数学上关于梯度下降的定义，本次视频我们更深入研究一下，更直观地感受一下这个算法是做什么的，以及梯度下降算法的更新过程有什么意义。梯度下降算法如下：

**${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta \right)$**

描述：对$\theta $赋值，使得$J\left( \theta  \right)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中$a$是**学习率（learning rate）**，它**决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大**。

![image-20240130122145562](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130122145562.png)

对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，让我们看看这条红色直线的斜率，就是这条刚好与函数曲线相切的这条直线，这条直线的斜率正好是这个三角形的高度除以这个水平长度，现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的${\theta_{1}}$，${\theta_{1}}$更新后等于${\theta_{1}}$减去一个正数乘以$a$。如果在左边则为负数，仍然在向最小值处下降。当我们接近最小值时，导数会自动变小，尽管变化率固定，也会使步子变小。

这就是我梯度下降法的更新规则：${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left( \theta  \right)$

让我们来看看如果$a$太小或$a$太大会出现什么情况：

如果$a$太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果$a$太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。

如果$a$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$a$太大，它会导致无法收敛，甚至发散。

现在，我还有一个问题，当我第一次学习这个地方时，我花了很长一段时间才理解这个问题，如果我们预先把${\theta_{1}}$放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？

假设你将${\theta_{1}}$初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得${\theta_{1}}$不再改变，也就是新的${\theta_{1}}$等于原来的${\theta_{1}}$，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率$a$保持不变时，梯度下降也可以收敛到局部最低点。

我们来看一个例子，这是代价函数$J\left( \theta  \right)$。

![image-20240130122232071](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130122232071.png)

我想找到它的最小值，首先初始化我的梯度下降算法，在那个品红色的点初始化，如果我更新一步梯度下降，也许它会带我到这个点，因为这个点的导数是相当陡的。现在，在这个绿色的点，如果我再更新一步，你会发现我的导数，也即斜率，是没那么陡的。随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所以，我再进行一步梯度下降时，我的导数项是更小的，${\theta_{1}}$更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。

回顾一下，在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以**当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度**，这就是梯度下降的做法。所以实际上没有必要再另外减小$a$。

这就是**梯度下降算法，可以用它来最小化任何代价函数$J$，不只是线性回归中的代价函数$J$。**

在接下来的视频中，我们要用代价函数$J$，回到它的本质，线性回归中的代价函数。也就是我们前面得出的平方误差函数，结合梯度下降法，以及平方代价函数，我们会得出第一个机器学习算法，即线性回归算法。



## 4.3 梯度下降的线性回归

在以前的视频中我们谈到关于梯度下降算法，梯度下降是很常用的算法，它不仅被用在线性回归上和线性回归模型、平方误差代价函数。在这段视频中，我们要将梯度下降和代价函数结合。我们将用到此算法，并将其应用于具体的拟合直线的线性回归算法里。

梯度下降算法和线性回归算法比较如图：

![image-20240130122430576](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130122430576.png)

对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

$\frac{\partial }{\partial {{\theta }_{j}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{\partial }{\partial {{\theta }_{j}}}\frac{1}{2m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}^{2}}$

$j=0$  时：$\frac{\partial }{\partial {{\theta }_{0}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}}$

$j=1$  时：$\frac{\partial }{\partial {{\theta }_{1}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

**则算法改写成：**

**Repeat {**
${\theta_{0}}:={\theta_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}$

 ${\theta_{1}}:={\theta_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

**}**

我们刚刚使用的算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”**批量梯度下降**”，指的是**在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有$m$个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。**在后面的课程中，我们也将介绍这些方法。

**此函数是凸函数，碗形函数，除了单个全局最小值以外，它不能有任何局部最小值。**

但就目前而言，应用刚刚学到的算法，你应该已经掌握了批量梯度算法，并且能把它应用到线性回归中了，这就是**用于线性回归的梯度下降法**。

如果你之前学过线性代数，有些同学之前可能已经学过高等线性代数，你应该知道有一种计算代价函数$J$最小值的数值解法，不需要梯度下降这种迭代算法。在后面的课程中，我们也会谈到这个方法，它可以在不需要多步梯度下降的情况下，也能解出代价函数$J$的最小值，这是另一种称为**正规方程(normal equations)**的方法。实际上**在数据量较大的情况下，梯度下降法比正规方程要更适用一些。**



# 5 线性代数回顾Linear Algebra

## 5.1 矩阵和向量

这个是4×2矩阵，即4行2列，如$m$为行，$n$为列，那么$m×n$即4×2

![image-20240130154746851](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240130154746851.png)

矩阵的维数即行数×列数

矩阵元素（矩阵项）：$A=\left[ \begin{matrix}   1402 & 191  \\   1371 & 821  \\   949 & 1437  \\   147 & 1448  \\\end{matrix} \right]$

$A_{ij}$指第$i$行，第$j$列的元素。

向量是一种特殊的矩阵，讲义中的向量一般都是列向量，如：

$y=\left[ \begin{matrix}   {460}  \\   {232}  \\   {315}  \\   {178}  \\\end{matrix} \right]$

为四维列向量（4×1）。

如下图为1索引向量和0索引向量，左图为1索引向量，右图为0索引向量，一般我们用1索引向量。

$y=\left[ \begin{matrix}   {{y}_{1}}  \\   {{y}_{2}}  \\   {{y}_{3}}  \\   {{y}_{4}}  \\\end{matrix} \right]$，$y=\left[ \begin{matrix}   {{y}_{0}}  \\   {{y}_{1}}  \\   {{y}_{2}}  \\   {{y}_{3}}  \\\end{matrix} \right]$



## 5.2 加法和标量乘法

矩阵的加法：行列数相等的可以加。

例：

![image-20240130160219811](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240130160219811.png)

矩阵的乘法：每个元素都要乘

![image-20240130160227072](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240130160227072.png)

组合算法也类似。



## 5.3 矩阵向量乘法

矩阵和向量的乘法如图：$m×n$的矩阵乘以$n×1$的向量，得到的是$m×1$的向量

![image-20240130160311995](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240130160311995.png)

算法举例：

![image-20240130160323054](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240130160323054.png)



## 5.4 矩阵乘法

矩阵乘法：

$m×n$矩阵乘以$n×o$矩阵，变成$m×o$矩阵。

如果这样说不好理解的话就举一个例子来说明一下，比如说现在有两个矩阵$A$和$B$，那么它们的乘积就可以表示为图中所示的形式。

![image-20240130160338943](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240130160338943.png)

![image-20240130160347905](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240130160347905.png)



## 5.5  矩阵乘法的性质

矩阵乘法的性质：

矩阵的乘法不满足交换律：$A×B≠B×A$

矩阵的乘法满足结合律。即：$A×(B×C)=(A×B)×C$

单位矩阵：在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 $I$ 或者 $E$ 表示，本讲义都用 $I$ 代表单位矩阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1以外全都为0。如：

$A{{A}^{-1}}={{A}^{-1}}A=I$

对于单位矩阵，有$AI=IA=A$



## 5.6 逆、转置

矩阵的逆：如矩阵$A$是一个$m×m$矩阵（方阵），如果有逆矩阵，则：$A{{A}^{-1}}={{A}^{-1}}A=I$

矩阵的转置：设$A$为$m×n$阶矩阵（即$m$行$n$列），第$i $行$j $列的元素是$a(i,j)$，即：$A=a(i,j)$

定义$A$的转置为这样一个$n×m$阶矩阵$B$，满足$B=a(j,i)$，即 $b (i,j)=a(j,i)$（$B$的第$i$行第$j$列元素是$A$的第$j$行第$i$列元素），记${{A}^{T}}=B$。(有些书记为A'=B）

直观来看，将$A$的所有元素绕着一条从第1行第1列元素出发的右下方45度的射线作镜面反转，即得到$A$的转置。

例：

${{\left| \begin{matrix}   a& b  \\   c& d  \\   e& f  \\\end{matrix} \right|}^{T}}=\left|\begin{matrix}   a& c & e  \\   b& d & f  \\\end{matrix} \right|$

矩阵的转置基本性质:

$ {{\left( A\pm B \right)}^{T}}={{A}^{T}}\pm {{B}^{T}} $
${{\left( A\times B \right)}^{T}}={{B}^{T}}\times {{A}^{T}}$
${{\left( {{A}^{T}} \right)}^{T}}=A $
${{\left( KA \right)}^{T}}=K{{A}^{T}} $

**matlab**中矩阵转置：直接打一撇，`x=y'`



# 6 多变量线性回归Linear Regression with Multiple Variables

## 6.1 多维特征

目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为$\left( {x_{1}},{x_{2}},...,{x_{n}} \right)$。

![image-20240201105740584](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201105740584.png)

增添更多特征后，我们引入一系列新的注释：

$n$ 代表**特征的数量**

${x^{\left( i \right)}}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个**向量**（**vector**）。

比方说，上图的

${x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}$，

**${x}_{j}^{\left( i \right)}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。**

如上图的$x_{2}^{\left( 2 \right)}=3,x_{3}^{\left( 2 \right)}=2$，

支持多变量的假设 $h$ 表示为：$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$，

这个公式中有$n+1$个参数和$n$个变量，为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：$h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$

此时模型中的参数是一个$n+1$维的向量(也可用向量表示)，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m*(n+1)$。 因此公式可以简化为：$h_{\theta} \left( x \right)={\theta^{T}}X$，其中上标$T$代表矩阵转置。

用向量来简化形式:

![image-20240201122308930](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201122308930.png)



## 6.2 矢量化

**使用矢量化既可以缩短代码，又可以提高运行效率**

我们来引用Python的Numpy库:

![image-20240201123606054](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201123608340.png)

Numpy dot函数利用并行硬件的能力使其更为高效



我们来看看矢量化在计算机背后做了什么

![image-20240201124050377](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201124050377.png)

**for循环分步，一步步地完成**

**而Numpy在计算机硬件中通过矢量化实现，计算机可以获得w和x的所有值，并在一步中同时并行地将每对w和x彼此相乘，再用专门的硬件高效地把它们相加。**

矢量化在处理大型数据集时显得极为高效



## 6.3 多变量梯度下降

矢量化以后可得：

![image-20240201150029026](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201150029026.png)



与单元线性回归的比较：

![image-20240201150222151](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201150222151.png)

与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：$J\left( {\theta_{0}},{\theta_{1}}...{\theta_{n}} \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}$ ，

其中：$h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$ ，

我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。`
`多变量线性回归的批量梯度下降算法为：

![image-20240201110059941](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201110059941.png)

即：

![image-20240201110113122](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201110113122.png)

`求导数后得到：`

![image-20240201110121954](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201110121954.png)

当$n>=1$时，`
`${{\theta }_{0}}:={{\theta }_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{0}^{(i)}$

${{\theta }_{1}}:={{\theta }_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{1}^{(i)}$

${{\theta }_{2}}:={{\theta }_{2}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}})}x_{2}^{(i)}$


我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。

代码示例：

计算代价函数$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$其中：${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$



Python 代码：

```python
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))
```



## 6.4 梯度下降法实践-特征缩放

在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法**更快**地收敛。

以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。

![image-20240201110943187](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201110943187.png)

解决的方法是** **。如图：

![image-20240201111001168](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201111001168.png)

**均值归一化**：最简单的方法是令：**${{x}_{n}}=\frac{{{x}_{n}}-{{\mu}_{n}}}{{{s}_{n}}}$，其中 ${\mu_{n}}$是平均值，${s_{n}}$是标准差。**



小技巧帮助判断何时应该特征缩放：

![image-20240201152357643](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201152357643.png)



## 6.5 梯度下降法实践-学习率

梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以**绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛**。

![image-20240201111449231](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201111449231.png)

**学习曲线**趋于水平，说明梯度下降几乎收敛了，因为曲线不再下降。

也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较（但这个阈值很难确定），但通常看上面这样的图表更好。



![image-20240201185511827](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201185511827.png)

**注意，如果图像成本有时上升有时下降，或者一直上升，应该将其视为梯度下降无法正常工作的明显迹象。可能意味着代码错误或者学习率太大了。当然，调整的时候也不要把学习率设置得太小**。

梯度下降算法的每次迭代受到学习率的影响，如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

通常可以考虑尝试这些学习率：

$\alpha=0.001，0，003，0.01，0.03，0.1，0.3，1，3，10$

尝试这样的一系列值，每一个后面的值都是前面的三倍左右，直到曲线刚好趋于水平。

## 6.6 特征工程和多项式回归

如房价预测问题:

![image-20240201112654416](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201112654416.png)

$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}\times{frontage}+{\theta_{2}}\times{depth}$

${x_{1}}=frontage$（临街宽度），${x_{2}}=depth$（纵向深度），$x=frontage*depth=area$（面积），则：${h_{\theta}}\left( x \right)={\theta_{0}}+{\theta_{1}}x$。`
`线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}$`
 `或者三次方模型： $h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}^2}+{\theta_{3}}{x_{3}^3}$


![image-20240201112736566](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201112736566.png)

通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：

${{x}_{2}}=x_{2}^{2},{{x}_{3}}=x_{3}^{3}$，从而将模型转化为线性回归模型。

根据函数图形特性，我们还可以使：

${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta}_{2}}{{(size)}^{2}}$

或者:

${{{h}}_{\theta}}(x)={{\theta }_{0}}\text{+}{{\theta }_{1}}(size)+{{\theta }_{2}}\sqrt{size}$

注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。



## 6.7 正规方程

到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：

![image-20240201112857672](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201112857672.png)

正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：**$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ 。**

假设我们的训练集特征矩阵为 $X$（包含了 ${{x}_{0}}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 。上标**T**代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A={X^{T}}X$，则：${{\left( {X^T}X \right)}^{-1}}={A^{-1}}$以下表示数据为例：


![image-20240201114512390](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201114512390.png)

`即：`

![image-20240201114523780](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201114523780.png)

`运用正规方程方法求解参数：`

![image-20240201114540106](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201114540106.png)



**注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。**

**梯度下降与正规方程的比较：**

| 梯度下降                      | 正规方程                                                     |
| ----------------------------- | ------------------------------------------------------------ |
| 需要选择学习率$\alpha$        | 不需要                                                       |
| 需要多次迭代                  | 一次运算得出                                                 |
| 当特征数量$n$大时也能较好适用 | 需要计算${{\left( {{X}^{T}}X \right)}^{-1}}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O\left( {{n}^{3}} \right)$，通常来说当$n$小于10000 时还是可以接受的 |
| 适用于各种类型的模型          | 只适用于线性模型，不适合逻辑回归模型等其他模型               |

![image-20240201150547531](C:\Users\x\AppData\Roaming\Typora\typora-user-images\image-20240201150547531.png)



**总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。**

随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，实际上对于那些算法，并不能使用标准方程法。**对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。**因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。

`正规方程的**python**实现：`

```python
import numpy as np
    
 def normalEqn(X, y):
    
   theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X等价于X.T.dot(X)
    
   return theta
```



## 6.8 正规方程及不可逆性（可选）?

我们要讲的问题如下：$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$`

备注：本节最后我把推导过程写下。h

有些同学曾经问过我，当计算 $\theta$=`inv(X'X ) X'y` ，那对于矩阵$X'X$的结果是不可逆的情况咋办呢?
如果你懂一点线性代数的知识，你或许会知道，有些矩阵可逆，而有些矩阵不可逆。我们称那些不可逆矩阵为奇异或退化矩阵。
问题的重点在于$X'X$的不可逆的问题很少发生，在**Octave**里，如果你用它来实现$\theta$的计算，你将会得到一个正常的解。在**Octave**里，有两个函数可以求解矩阵的逆，一个被称为`pinv()`，另一个是`inv()`，这两者之间的差异是些许计算过程上的，一个是所谓的伪逆，另一个被称为逆。使用`pinv()` 函数可以展现数学上的过程，这将计算出$\theta$的值，即便矩阵$X'X$是不可逆的。

在`pinv()` 和 `inv()` 之间，又有哪些具体区别呢 ?

其中`inv()` 引入了先进的数值计算的概念。例如，在预测住房价格时，如果${x_{1}}$是以英尺为尺寸规格计算的房子，${x_{2}}$是以平方米为尺寸规格计算的房子，同时，你也知道1米等于3.28英尺 ( 四舍五入到两位小数 )，这样，你的这两个特征值将始终满足约束：${x_{1}}={x_{2}}*{{\left( 3.28 \right)}^{2}}$。
实际上，你可以用这样的一个线性方程，来展示那两个相关联的特征值，矩阵$X'X$将是不可逆的。

第二个原因是，在你想用大量的特征值，尝试实践你的学习算法的时候，可能会导致矩阵$X'X$的结果是不可逆的。
具体地说，在$m$小于或等于n的时候，例如，有$m$等于10个的训练样本也有$n$等于100的特征数量。要找到适合的$(n +1)$ 维参数矢量$\theta$，这将会变成一个101维的矢量，尝试从10个训练样本中找到满足101个参数的值，这工作可能会让你花上一阵子时间，但这并不总是一个好主意。因为，正如我们所看到你只有10个样本，以适应这100或101个参数，数据还是有些少。

稍后我们将看到，如何使用小数据样本以得到这100或101个参数，通常，我们会使用一种叫做正则化的线性代数方法，通过删除某些特征或者是使用某些技术，来解决当$m$比$n$小的时候的问题。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。
总之当你发现的矩阵$X'X$的结果是奇异矩阵，或者找到的其它矩阵是不可逆的，我会建议你这么做。

首先，看特征值里是否有一些多余的特征，像这些${x_{1}}$和${x_{2}}$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。因此，首先应该通过观察所有特征检查是否有多余的特征，如果有多余的就删除掉，直到他们不再是多余的为止，如果特征数量实在太多，我会删除些 用较少的特征来反映尽可能多内容，否则我会考虑使用正规化方法。
如果矩阵$X'X$是不可逆的，（通常来说，不会出现这种情况），如果在**Octave**里，可以用伪逆函数`pinv()` 来实现。这种使用不同的线性代数库的方法被称为伪逆。即使$X'X$的结果是不可逆的，但算法执行的流程是正确的。总之，出现不可逆矩阵的情况极少发生，所以在大多数实现线性回归中，出现不可逆的问题不应该过多的关注${X^{T}}X$是不可逆的。

**增加内容：**

$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$ 的推导过程：

$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$
其中：${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$

将向量表达形式转为矩阵表达形式，则有$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{2}}$ ，其中$X$为$m$行$n$列的矩阵（$m$为样本个数，$n$为特征个数），$\theta$为$n$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\theta )$进行如下变换

$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{T}}\left( X\theta -y \right)$

     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}-{{y}^{T}} \right)\left(X\theta -y \right)$
    
     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}X\theta -{{\theta}^{T}}{{X}^{T}}y-{{y}^{T}}X\theta -{{y}^{T}}y \right)$

接下来对$J(\theta )$偏导，需要用到以下几个矩阵的求导法则:

$\frac{dAB}{dB}={{A}^{T}}$ 

$\frac{d{{X}^{T}}AX}{dX}=2AX$                            

所以有:

$\frac{\partial J\left( \theta  \right)}{\partial \theta }=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{}({{y}^{T}}X )^{T}-0 \right)$

$=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{{X}^{T}}y -0 \right)$

           $={{X}^{T}}X\theta -{{X}^{T}}y$

令$\frac{\partial J\left( \theta  \right)}{\partial \theta }=0$,

则有$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$



